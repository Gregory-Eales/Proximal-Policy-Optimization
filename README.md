   ---   
<div align="center">    
 
# Proximal Policy Optimization    

[![Paper](http://img.shields.io/badge/paper-arxiv.1001.2234-B31B1B.svg)](https://www.nature.com/articles/nature14539)
[![Conference](http://img.shields.io/badge/NeurIPS-2019-4b44ce.svg)](https://papers.nips.cc/book/advances-in-neural-information-processing-systems-31-2018)
[![Conference](http://img.shields.io/badge/ICLR-2019-4b44ce.svg)](https://papers.nips.cc/book/advances-in-neural-information-processing-systems-31-2018)
[![Conference](http://img.shields.io/badge/AnyConference-year-4b44ce.svg)](https://papers.nips.cc/book/advances-in-neural-information-processing-systems-31-2018)  
<!--
ARXIV   
[![Paper](http://img.shields.io/badge/arxiv-math.co:1480.1111-B31B1B.svg)](https://www.nature.com/articles/nature14539)
-->



<!--  
Conference   
-->   
</div>
 
## Description   
This is a reimplementation of the proximal policy algorithm 

## How to run   
First, install dependencies   
```bash
# clone project   
git clone https://github.com/Gregory-Eales/Proximal-Policy-Optimization 

# install project   
cd Proximal-Policy-Optimization
pip install -e .   
pip install -r requirements.txt
 ```   
 Next, navigate to ppo and run it.   
 ```bash
# module folder
cd src/    

# train poo 
python train.py    

# run ppo   
python run.py    
``` 


## Baselines    
List your baselines here.   
- [PPO Baseline]

### Citation   
```
@article{YourName,
  title={Your Title},
  author={Your team},
  journal={Location},
  year={Year}
}
```   
